{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Name:** \\_\\_\\_\\_\\_\n",
    "\n",
    "**EID:** \\_\\_\\_\\_\\_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# CS5495 - Tutorial 5\n",
    "## CNNs - Feature Visualization\n",
    "\n",
    "In this tutorial, you use feature visualization to examine the learned features in a CNN. \n",
    "\n",
    "First we need to initialize Python.  Run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "%matplotlib inline\n",
    "import matplotlib_inline   # setup output image format\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 100  # display larger images\n",
    "import matplotlib\n",
    "from numpy import *\n",
    "from sklearn import *\n",
    "from scipy import stats\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "pd.set_option('display.precision', 5)\n",
    "import statsmodels.api as sm\n",
    "import lime\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "\n",
    "print(f\"Using pytorch version: {torch.__version__}\")\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"Using: {torch.cuda.get_device_name(device)}\")\n",
    "elif (torch.backends.mps.is_available()):\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using: Apple MPS\")\n",
    "else:\n",
    "    raise(\"no GPU available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Helper functions\n",
    "- These are helper functions from the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_imgs(W_list, nc=10, highlight_green=None, highlight_red=None, titles=None):\n",
    "    nfilter = len(W_list)\n",
    "    nr = (nfilter - 1) // nc + 1\n",
    "    for i in range(nr):\n",
    "        for j in range(nc):\n",
    "            idx = i * nc + j\n",
    "            if idx == nfilter:\n",
    "                break\n",
    "            plt.subplot(nr, nc, idx + 1)\n",
    "            cur_W = W_list[idx]\n",
    "            plt.imshow(cur_W,cmap='gray', interpolation='nearest')  \n",
    "            if titles is not None:\n",
    "                if isinstance(titles, str): \n",
    "                    plt.title(titles % idx)\n",
    "                else:\n",
    "                    plt.title(titles[idx])\n",
    "            \n",
    "            if ((highlight_green is not None) and highlight_green[idx]) or \\\n",
    "               ((highlight_red is not None) and highlight_red[idx]): \n",
    "                ax = plt.gca()\n",
    "                if highlight_green[idx]:\n",
    "                    mycol = '#00FF00'\n",
    "                else:\n",
    "                    mycol = 'r'\n",
    "                for S in ['bottom', 'top', 'right', 'left']:\n",
    "                    ax.spines[S].set_color(mycol)\n",
    "                    ax.spines[S].set_lw(2.0)\n",
    "                ax.xaxis.set_ticks_position('none')               \n",
    "                ax.yaxis.set_ticks_position('none')\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "            else:\n",
    "                plt.gca().set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Dog vs Cat Dataset\n",
    "\n",
    "The task is to classify an image as having a cat or a dog. This dataset is from [Kaggle](https://www.kaggle.com/competitions/dogs-vs-cats-redux-kernels-edition/data).\n",
    "\n",
    "In our dataset Class `0` is cat, and class `1` is dog.\n",
    "\n",
    "First let's load the validation images. Make sure you have unzipped the validation and model files into your directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the transform of the data for input into the network\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to store the dataset\n",
    "class CatDogDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform):\n",
    "        super().__init__()\n",
    "        self.paths = image_paths\n",
    "        self.len = len(self.paths)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self): return self.len\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        path = self.paths[index]\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        label = 0 if 'cat' in path else 1\n",
    "        return (image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect validation data file names\n",
    "img_files = os.listdir('valid/')\n",
    "len(img_files)\n",
    "img_files = list(filter(lambda x: x != 'valid', img_files))\n",
    "def valid_path(p): return f\"valid/{p}\"\n",
    "img_files = list(map(valid_path, img_files))[::10] # subsample to make it faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "valid_ds = CatDogDataset(img_files, test_transform)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=100)\n",
    "classnames = ['cat', 'dog']\n",
    "len(valid_ds), len(valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's view a few samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_imgs  = []\n",
    "eg_names = []\n",
    "for X,Y in valid_dl:\n",
    "    for i in range(15):\n",
    "        # transform the image from [-1,1] to [0,1]\n",
    "        eg_imgs.append( (1+transpose(X[i], (1,2,0)))/2 )\n",
    "        eg_names.append( classnames[Y[i]] )\n",
    "    break\n",
    "plt.figure(figsize=(10,7))\n",
    "show_imgs(eg_imgs, titles=eg_names, nc=5)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Deep CNNs\n",
    "\n",
    "We have trained four Deep CNNs on the training set:\n",
    "\n",
    "1. Model 1: 5 layers of convolutions, then global-average pooling and a linear classifier layer.\n",
    "2. Model 2: 7 layers of convolutions, then global average pooling and a linear classifier layer.\n",
    "3. Model 3: ResNet-18 (17 convolution layers), then global average pooling and a linear classifier layer. Trained from scratch.\n",
    "4. Model 4: ResNet-18 (same as Model 3), but the network is initialized with pre-trained weights based on ImageNet.\n",
    "\n",
    "Note that the global-average pooling takes the last convolution feature map (say C x H x W), and then averages over all the spatial nodes to obtain a (C x 1 x 1) feature vector. This is then used by the linear classifier layer.  Thus, the weights in the linear classifier layer will indicate which of the C feature channels was useful for the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load each model. If your GPU has limited memory, probably you should only load one model at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 (5 conv layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatAndDogNet4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size=(5, 5), stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size=(5, 5), stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size=(3, 3), padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size=(3, 3), padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size=(3, 3), padding=1)\n",
    "        self.gap   = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features= 128, out_features=2)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.max_pool2d(X, 2)\n",
    "        \n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X, 2)\n",
    "        \n",
    "        X = F.relu(self.conv3(X))\n",
    "        X = F.max_pool2d(X, 2)\n",
    "\n",
    "        X = F.relu(self.conv4(X))\n",
    "        \n",
    "        X = F.relu(self.conv5(X))\n",
    "\n",
    "        X = self.gap(X)\n",
    "        \n",
    "        X = X.view(X.shape[0], -1)\n",
    "        X = self.fc1(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model1 = CatAndDogNet4().to(device)\n",
    "model1.load_state_dict(torch.load('models/model4-19.pth', map_location=\"cpu\"))\n",
    "model1.to(device).eval()\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 (7 conv layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatAndDogNet6(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size=(5, 5), stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size=(5, 5), stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size=(3, 3), padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size=(3, 3), padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size=(3, 3), padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size=(3, 3), padding=1)\n",
    "        self.conv7 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size=(3, 3), padding=1)\n",
    "        self.gap   = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(in_features= 128, out_features=2)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.max_pool2d(X, 2)\n",
    "        \n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X, 2)\n",
    "        \n",
    "        X = F.relu(self.conv3(X))\n",
    "        X = F.max_pool2d(X, 2)\n",
    "\n",
    "        X = F.relu(self.conv4(X))\n",
    "        \n",
    "        X = F.relu(self.conv5(X))\n",
    "\n",
    "        X = F.relu(self.conv6(X))\n",
    "\n",
    "        X = F.relu(self.conv7(X))\n",
    "        \n",
    "        X = self.gap(X)\n",
    "        \n",
    "        X = X.view(X.shape[0], -1)\n",
    "        X = self.fc1(X)        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model2 = CatAndDogNet6().to(device)\n",
    "model2.load_state_dict(torch.load('models/model6-19.pth', map_location=\"cpu\"))\n",
    "model2.to(device).eval()\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 (ResNet-18, from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = models.resnet18().to(device)\n",
    "in_feats = model3.fc.in_features\n",
    "model3.fc = nn.Linear(in_feats, 2)\n",
    "model3.load_state_dict(torch.load('models/model8-17.pth', map_location=\"cpu\"))\n",
    "model3.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4 (ResNet-18, pretrained on ImageNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = models.resnet18().to(device)\n",
    "in_feats = model4.fc.in_features\n",
    "model4.fc = nn.Linear(in_feats, 2)\n",
    "model4.load_state_dict(torch.load('models/model7-18.pth', map_location=\"cpu\"))\n",
    "model4.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Now we will evaluate the models. First consolidate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = {\n",
    "    'CNN5': model1, \n",
    "    'CNN7': model2, \n",
    "    'ResNet18': model3,\n",
    "    'ResNet18p': model4\n",
    "}\n",
    "modelnames = all_models.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate each  model on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "correct = {}\n",
    "total = {}\n",
    "for myname,mymodel in all_models.items():    \n",
    "    mymodel.to(device).eval()\n",
    "    correct[myname] = 0\n",
    "    total[myname] = 0\n",
    "    \n",
    "with torch.no_grad():\n",
    "    # for each validation batch\n",
    "    for data, targets in valid_dl:\n",
    "        print('.', end='', flush=True)                \n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "        # test each model\n",
    "        for myname,mymodel in all_models.items():    \n",
    "            mymodel.to(device).eval()\n",
    "            correct[myname] = 0\n",
    "            total[myname] = 0\n",
    "        \n",
    "            outputs = mymodel(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total[myname] += targets.size(0)\n",
    "            correct[myname] += (predicted == targets).sum().item()\n",
    "\n",
    "print(\"\")\n",
    "for myname in all_models.keys():    \n",
    "    print(myname + f' Accuracy on valid set: {100 * correct[myname] / total[myname]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Visualization\n",
    "\n",
    "Now, examine the features of each model using the feature visualization method. \n",
    "\n",
    "You can use the `lucent` toolbox. If it is not installed on your system, use the following command: `pip install torch-lucent`\n",
    "On the CS JupyterLab, you can run this by opening a terminal tab. In Jupyter notebook, you can run the following \"magic\" command `!pip install torch-lucent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-lucent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features in the Last Conv Layer \n",
    "\n",
    "Perform feature visualization on the last convolutional layer of the networks.  Since there are a lot of features, one suggestion is to focus on those features that were more useful for the classifier (e.g., looking at the features with largest effects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Summary of Feature Visualization\n",
    "\n",
    "Provide a summary of your interpretation of the models using feature visualization. Some interesting questions to consider...\n",
    "- Considering the four models, what types are features are extracted from the last conv layers?\n",
    "- How do the features change with the depth of the model?\n",
    "- How are the features different for models learned from scratch versus using a pre-trained initialization?\n",
    "- how does the feature visualization help to understand the differences in validation set accuracy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "- **INSERT YOUR ANSWER HERE**\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T14:32:27.922108Z",
     "iopub.status.busy": "2025-10-03T14:32:27.921740Z",
     "iopub.status.idle": "2025-10-03T14:32:27.926961Z",
     "shell.execute_reply": "2025-10-03T14:32:27.926063Z",
     "shell.execute_reply.started": "2025-10-03T14:32:27.922084Z"
    }
   },
   "source": [
    "# Art!\n",
    "\n",
    "Now explore the feature visualizations of other parts of the network. See if you can find some interesting textures or patterns. What do they resemble?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
